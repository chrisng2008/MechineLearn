[toc]

# 第一章 绪论

1. 根据每一个特征不同的取值，在向量空间中对应一个向量坐标，我们把这个坐标成为“特征向量”
2. 从数据中学的模型的过程成为学习(Learning)或者训练(Training)，这个过程通过执行某个学习算法来完成。
3. 预测的是离散值，为分类模型；预测的是连续值，为回归模型。分类又分成二分类和多分类。
4. 根据训练集是否有标记信息，学习任务可以分成“监督学习”和“无监督学习”。
5. 我们希望学习的模型更好适应“新样本”。学习模型适用于新样本的能力，成为“泛化“能力。

### 假设空间

归纳是从特殊到一般的"泛化"过程；演绎是从一般到特殊的“特化”过程。

现实问题中，可能有多个假设与训练集一直，即存在着一个与训练集一致的“假设”集合，我们称之为“版本空间”。

机器学习中对某种类型的假设的偏好，称之为“归纳偏好”，或简称为“偏好”。对于具体选取哪一种特征，我们通常采用“奥姆特剃刀(Occam's razor)”，即有多个加社与观察一致，则选最简单的那个。如果采用此原则，并且假设认为“更平滑”意味着“更简单”，那么我们通常偏好于更平滑的曲线。

$$\sum_{f}Eote(\xi_{a}|X,f)=2^{|\chi|-1}\sum_{x\in \chi-X}P(x)*1$$

因此，总误差与学习算法无关。要讨论算法的优劣性，必须要针对具体的学习问题。