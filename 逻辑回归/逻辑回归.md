# 逻辑回归 Llgistic Regression

逻辑回归：解决分类问题，将样本的特征和样本发生的概率联系起来，概率是一个数。

 逻辑回归既可以看作回归算法，也可以看作分类算法
通常作为分类算法用，只可以解决**二分类问题** 

$\hat y=\theta^T \cdot x_b$ 令值域为[0,1]，我们采用的方法是 
$$
\hat p = \sigma(\theta^T \cdot x_b)\\
\sigma(t)=\frac{1}{1+e^{-t}}
$$
$\sigma(t)$的值域在0-1之间
$$
t>0时，p>0.5\\
t<0时，p<0.5
$$

$$
\sigma(t)=\sigma(\theta^T \cdot x_b)=\frac{1}{1+e^{-\theta^T \cdot x_b}}
$$

## 逻辑回归的损失函数

$$
f(x)= \begin{cases} 
1&	\hat p > 0.5\\ 
0&  \hat p < 0.5
\end{cases}
$$

$$
cost=\begin{cases}
& 如果y=1，p越小，cost越大\\
& 如果y=0，p越大，cost越大
\end{cases}
$$

$$
cost=\begin{cases}
-log(\hat p)& if\ \ y=1\\
-log(1-\hat p)& if\ \ y=0
\end{cases}
$$

以上损失函数我们可以写为
$$
cost = -ylog(\hat p)-(1-y)log(1-\hat p)
$$

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(\hat p^{(i)})+(1-y^{(i)})log(1-\hat p^{(i)})\\
\hat p^{(i)}=\sigma(\theta^T \cdot X_b)=\frac{1}{1+e^{-  X_b\cdot \theta}}
$$

综上
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(\sigma(X_b^{(i)} \cdot \theta))+(1-y^{(i)})log(1-\sigma(X_b^{(i)}\cdot \theta))
$$
对于这个公式我们没有正规方程解，我们只能用梯度下降法来求解

## 逻辑回归损失函数的梯度

$$
\sigma(t)==\frac{1}{1+e^{-t}}\\
\sigma(t)^{'}=(1+e^{-t})^{-2}\cdot e^{-t}
$$

$$
\begin{align}
(log(\sigma(t))^{'} &= \frac{1}{(\sigma(t)}\cdot(1+e^{-t})^{-2}\cdot e^{-t}\\
&= \frac{1}{(1+e^{-t})^{-1}}(1+e^{-t})^{-2} \cdot e^{-t}\\
&=(1+e^{-t})^{-1}\cdot e^{-t}\\
&=\frac{e^{-t}}{1+e^{-t}}\\
&=\frac{1+e^{-t}-1}{1+e^{-t}}\\
&=1-\frac{1}{1+e^{-t}}\\
&= 1-\sigma(t)
\end{align}

$$

同理可得
$$
(log(1-\sigma(t)))^{'}=\frac{1}{1-\sigma(t)}(-1)\cdot \sigma(t)^{'}=-\frac{1}{1-\sigma(t)}\cdot(1+e^{-t})^{-2}\cdot e^{-t}-\frac{1}{1-\sigma(t)}=-\frac{1+e^{-t}}{e^{-t}}
$$
