# 逻辑回归 Llgistic Regression

逻辑回归：解决分类问题，将样本的特征和样本发生的概率联系起来，概率是一个数。

 逻辑回归既可以看作回归算法，也可以看作分类算法
通常作为分类算法用，只可以解决**二分类问题** 

$\hat y=\theta^T \cdot x_b$ 令值域为[0,1]，我们采用的方法是 
$$
\hat p = \sigma(\theta^T \cdot x_b)\\
\sigma(t)=\frac{1}{1+e^{-t}}
$$
$\sigma(t)$的值域在0-1之间
$$
t>0时，p>0.5\\
t<0时，p<0.5
$$

$$
\sigma(t)=\sigma(\theta^T \cdot x_b)=\frac{1}{1+e^{-\theta^T \cdot x_b}}
$$

## 逻辑回归的损失函数

$$
f(x)= \begin{cases} 
1&	\hat p > 0.5\\ 
0&  \hat p < 0.5
\end{cases}
$$

$$
cost=\begin{cases}
& 如果y=1，p越小，cost越大\\
& 如果y=0，p越大，cost越大
\end{cases}
$$

$$
cost=\begin{cases}
-log(\hat p)& if\ \ y=1\\
-log(1-\hat p)& if\ \ y=0
\end{cases}
$$

以上损失函数我们可以写为
$$
cost = -ylog(\hat p)-(1-y)log(1-\hat p)
$$

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(\hat p^{(i)})+(1-y^{(i)})log(1-\hat p^{(i)})\\
\hat p^{(i)}=\sigma(\theta^T \cdot X_b)=\frac{1}{1+e^{-  X_b\cdot \theta}}
$$

综上
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(\sigma(X_b^{(i)} \cdot \theta))+(1-y^{(i)})log(1-\sigma(X_b^{(i)}\cdot \theta))
$$
对于这个公式我们没有正规方程解，我们只能用梯度下降法来求解

## 逻辑回归损失函数的梯度

$$
\sigma(t)==\frac{1}{1+e^{-t}}\\
\sigma(t)^{'}=(1+e^{-t})^{-2}\cdot e^{-t}
$$

$$
\begin{align}
(log(\sigma(t))^{'} &= \frac{1}{(\sigma(t)}\cdot(1+e^{-t})^{-2}\cdot e^{-t}\\
&= \frac{1}{(1+e^{-t})^{-1}}(1+e^{-t})^{-2} \cdot e^{-t}\\
&=(1+e^{-t})^{-1}\cdot e^{-t}\\
&=\frac{e^{-t}}{1+e^{-t}}\\
&=\frac{1+e^{-t}-1}{1+e^{-t}}\\
&=1-\frac{1}{1+e^{-t}}\\
&= 1-\sigma(t)
\end{align}

$$

同理可得
$$
(log(1-\sigma(t)))^{'}=\frac{1}{1-\sigma(t)}(-1)\cdot \sigma(t)^{'}=-\frac{1}{1-\sigma(t)}\cdot(1+e^{-t})^{-2}\cdot e^{-t}\\
-\frac{1}{1-\sigma(t)}=-\frac{1+e^{-t}}{e^{-t}}
$$
因此
$$
\begin{align}
(log(1-\sigma(t)))^{'} &= -\frac{1+e^{-t}}{e^{-t}}\cdot(1+e^{-t})^{-2}\cdot e^{-t}\\
&= -(1+e^{-t})^{-1}\\
&=-\sigma(t)
\end{align}
$$


所以对$j(\theta)$求导可得：
$$
\begin{align}
\frac{\partial J(\theta)}{\partial \theta_{j}}&=\frac{1}{m} \sum_{i=1}^m(\sigma(X_b^{(i)}\theta)-y^{(i)})\cdot X_j^{(i)}\\
&=\frac{1}{m}\sum_{i=1}^m(\hat y^{(i)}-y^{(i)})X_j^{(i)}
\end{align}
$$


梯度就是分别对每一个$\theta$进行求偏导从而得到的向量。

向量化表示：
$$
\triangledown J(\theta)=\frac{1}{m}\cdot X_b^T(\sigma(X_b \theta)-y)
$$

 ## 算法实现



```python
import numpy as np
from .metrics import accuracy_score

class LogisticRegression:
    def __init__(self):
        '''初始化Logistic Regression模型'''
        self.coef_ = None
        self.intercept_=None
        self._theta = None
        
    def _sigmoid(self, t):
        return 1./(1.+np.exp(-t))
    
    def fit(self, X_train, y_trian, eta=0.01, n_iters=1e4):
        '''根据训练数据集X_train, t_train, 使用梯度下降算法训练Logistic Regreesion模型'''
        assert X_train.shape[0] == y_train.shape[0],\
        "the size of X_train must be equal to the size of y_train"
        
        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b, y)
            
        
```



