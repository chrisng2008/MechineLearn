[toc]

# 第三章 线性模型

线性模型试图学得一个通过属性的线性组合来进行预测的函数，即
$$
f(x)=w_1x_1+w_2x_2+\dots+w_dx_d+b
$$
一般用向量形式写成
$$
f(x)=w^Tx+b\\
其中w=(w_1;w_2;\dots;w_d)
$$
如何确定w、b，均方误差是回归任务中最常用的性能度量，我们试图让均方误差最小化，即
$$
\begin{align}
(w^*,b^*)&={arg\ min}_{(w,b)}\sum_{i=1}^{m}(f(x_i)-y_i)^2\\
&={arg\ min}_{(w,b)}\sum_{i=1}^{m}(y_i-wx_i-b)^2
\end{align}
$$
在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的**欧式距离**之和最小。

求解w和b使$E_{(w,b)}=\sum_{i=1}^{m}(y_i-wx_i-b)^2$最小化的过程，称为线性回归模型的最小二乘的“参数估计”。我们分别对w和b求偏导
$$
\frac{\partial E_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^{m}x_i^2-\sum_{i=1}{m}(y_i-b)x_i)\\
\frac{\partial E_{(w,b)}}{\partial b} = 2(mb-\sum_{i=1}^{m}(y_i-wx_i))
$$
令上两式为零，即可得到w和b的最优解w和b的最优解的闭式解
$$
w = \frac{\sum_{i=1}^{m}y_i(x_i-\bar x )}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_1)^2}\\
b=\frac{1}{m}\sum_{i=1}{m}(y_i-wx_i)\\
其中，\bar x = \frac{1}{m}\sum_{i=1}^{m}x_i的均值
$$

## 线性判别分析

线性判别分析，简称LDA，是一种经典的线性学习方法。

LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。

