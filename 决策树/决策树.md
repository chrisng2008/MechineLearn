[toc]

---

# Decision Tree

决策树是基于**树结构**来进行决策的，是常见的**分类**模型。

决策树学习基本算法

```
输入：训练集 D = {(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}
	 属性集 A = {a_1,a_2,...,a_d}
过程：函数TreeGenerate(D,A)
1:生成结点node
2:if D 中样本全属于同一类别C then
	将node标记为C类叶结点；return
end if
if A = None OR D中样本在A上取值相同 then
	将node标记为叶节点，其类别标记为D中样本数最多的类；return
end if
从A中选择最优划分属性a_*;
for a_*的每一个值a_{*}^{v} do
	为node生成一个分支；令D_v表示D中在a_*上取值为a_{*}^{v}的样本子集;
```

## 划分选择

### 信息增益

信息熵定义：假设当前集合d中第 $k$ 类样本所占的比例为$p_k(k=1,2,\dots,|y|)$, 则信息熵的定义为
$$
Ent(D)=-\sum_{k=1}^{|y|}p_k \log_{2}p_k.
$$
$Ent(D)$的值越小，则D的纯度越高。

考虑到不同的分支结点所包含的样本数不同，跟分支结点赋予权重$|D^v|/|D|$,即样本数越多的分支影响越大，于是可计算出用属性a对样本集D进行划分所获得的“信息增益”(infomation gain)
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v).
$$

### 基尼系数



## 剪枝处理

剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段。通过主动去掉一些分支来降低过拟合的风险。

### 预剪枝

预剪枝是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶节点；

### 后剪枝

后剪枝是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点对应的子树替换为叶节点能带来决策树性能泛化性能提升，则将该子树替换为叶结点。

## 连续与缺失值

### 连续值处理

现实学习任务中常会遇到连续属性。由于连续属性的可取值数目不在有限。不能直接根据连续属性的可取值来对结点进行划分。最简单的策略是采用二分法对连续属性进行处理。这是C4.5决策树算法中采用的机制。
$$
T_a=\left\{ \frac{a^i+a^{i+1}}{2}|1\leq i \leq n-1 \right\}
$$
即我们把区间$[a^i,a^{i+1})$的中位点$\frac{a^i+a^{i+1}}{2}$作为候选划分点
$$
\begin{align}
Gain(D,a) &= \mathop{max}\limits_{t \in {T_a}}Gain(D,a,t)\\
&= \mathop{max}\limits_{t \in {T_a}}Ent(D)-\sum_{\lambda \in \left \{ -,+ \right \}}\frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda})
\end{align}
$$
其中Gain(D,a,t)是样本集D基于划分点t二分后的信息增益。于是，我们就选择使Gain(D,a,t)最大化的划分点。

### 缺失值处理

## 多变量决策树



## 决策树划分

### ID3 决策树

ID3决策树用可用信息增益来进行决策树的划分属性选择。



### C4.5决策树

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树不直接采用信息增益，而是使用“增益率”来选择最优划分属性。“增益率”定义为


$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$

