[toc]

# 评价分类结果

对于不同的分类结果，我们有不同的评估标准。

## 混淆矩阵 Confusion Matrix

|      | 0                            | 1                            |
| ---- | ---------------------------- | ---------------------------- |
| 0    | 预测negative正确<br />**TN** | 预测positive错误<br />**FP** |
| 1    | 预测negative错误<br />**FN** | 预测positive正确<br />**TP** |

- 行代表真实值 [axis=0]

- 列代表预测值 [axis=1]

  - 0 - Negative
  - 1 - Positive

   

## 精准率与召回率



| 真实 \ 预测 | 0                            | 1                            |
| ----------- | ---------------------------- | ---------------------------- |
| 0           | 预测negative正确<br />**TN** | 预测positive错误<br />**FP** |
| 1           | 预测negative错误<br />**FN** | 预测positive正确<br />**TP** |

精准率： 在预测的数据中，预测正确的是多少
$$
precision=\frac{TP}{TP+FP}
$$
召回率：在真实的数据中，我们成功预测了多少
$$
recall=\frac{TP}{TP+FN}
$$


![350px-Precisionrecall.svg](350px-Precisionrecall.svg.png)

Precision和Recall可以用上图来表示。

对于以下数据

| 真实\预测 | 0    | 1    |
| --------- | ---- | ---- |
| 0         | 9990 | 0    |
| 1         | 10   | 0    |

准确率 = 99.9%<br />精准率 = 0 / (0 + 0) 无意义<br />召回率 = 0 / (10 + 0) = 0

如果我们只是看准确率，算法的准确率达到99.9%，但是实际上这个算法是不合理的，所以，对于此类问题，我们要用



## 代码实现

```python
def TN(y_true, y_predict):
    assert len(y_true)==len(y_predict)
    return np.sum((y_true==0)&&(y_predict==0))

def FP(y_true, y_predict):
    assert len(y_true)==len(y_predict)
    return np.sum((y_true==0)&&(y_predict==1))

def FN(y_true, y_predict):
    assert len(y_true)==len(y_predict)
    return np.sum((y_true==1)&&(y_predict==0))

def TP(y_true, y_predict):
    assert len(y_true)==len(y_predict)
    return np.sum((y_true==1)&&(y_predict==1))
```

```python
def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true,y_predict),FP(y_true,y_predict)],
        [FN(y_true,y_predict),TP(y_true,y_predict)]
    ])
```

```python
def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp+fp)
    except:
        return 0.0

def recall_score(y_true, y_predict):
    fn = FN(y_true, y_predict)
    tp = TP(y_true, y_predict)
    try:
        return tp / (tp+fn)
    except:
        return 0.0
```

## sklearn中的混淆矩阵，精准率，召回率

### 混淆矩阵

```python
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_predict)
```

### 精准率和召回率

```python
from sklearn.metrics import precision_score
precision_score(y_test, y_predict)

from sklearn.metrics import recall_score
recall_score(y_test, y_predict)
```

## 精准率和找回率的应用场景

对于股票预测，我们更加很注重精准率<br />对于医疗病人预测，我们更加注重召回率

那有没有什么可以更好的指标来衡量呢？

 ### F1 Score

二者都兼顾:F1 Score
$$
F1=\frac{2\cdot precision \cdot recall}{precision + recall}\\
F1 \in [0,1]
$$
F1 score 是precision和recall的调和平均值
$$
\frac{1}{F1}=\frac{1}{2}(\frac{1}{precision}+\frac{1}{recall})
$$

```python
def f1_score(precision,recall):
    try:
        return (2*precision*recall)/(precision+recall)
    except:
        return 0.0
```



## Precision-Recall 的平衡 

 对于逻辑回归来说，决策边界是$\theta^T X=0$为决策边界，如果我们修改$\theta^T X=threshold$，召回率和精准率会变化。精准率变高，召回率会降低。

 ```python
 y_predict = np.array(decision_scores>=-5, detype='int')
 ```



```python
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

decision_scores = log_reg.decision_function(X_test)
precisions=[]
recalls=[]
thresholds=np.arange(np.min(decision_scores),np.min(decision_scores),0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores>=threshold, detype='int')
    precisions.append(precision_score(y_test,y_predict))
    recalls.append(recall_score(y_test,y_predict))
```



## ROC 曲线

ROC (Receiver Operation Characteristic Curve)<br />描述TPR和FPR之间的关系



| 真实 \ 预测 | 0                            | 1                            |
| ----------- | ---------------------------- | ---------------------------- |
| 0           | 预测negative正确<br />**TN** | 预测positive错误<br />**FP** |
| 1           | 预测negative错误<br />**FN** | 预测positive正确<br />**TP** |

- TPR  (实际是召回率)

  - $$
    TPF = \frac{TP}{TP+FN}
    $$

- FPR

  - $$
    FPR=\frac{FP}{TN+FP}
    $$

TPR越高，FPR越高

根据公式我们可以写出FPR和TPR的函数，这里省略。

```python
from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, decision_score)
```



### 使用场合

在选择更优模型的时候使用。



## 多分类问题中的混淆矩阵

```python
from sklearn.metrics import precision_score

precision_score(y_test, y_predict, average='micro')
```

混淆矩阵支持多分类问题

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
```

